{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Rr22S6YP4IAn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WQWfsj_y8SZ"
      },
      "source": [
        "# ***Code explanation*** \n",
        "\n",
        "The data is initially loaded, then cropped and augmented (data augmentation and edges augmentation). Then, the model (U-Net) is set up and trained. Finally, test data is loaded and compared with U-Net prediction.\n",
        "\n",
        "Note: data loading and cropping in this notebook may be redundant with the notebook \"preprocess_data\" (which is mainly used for change detection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxgSX4Vyc0Bj"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.io as tfio\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision as tv\n",
        "import torch\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ktYk71mbKxd"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF2m0iDDeAMf"
      },
      "source": [
        "!ls drive/My\\ Drive/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaAZXalsc0B8"
      },
      "source": [
        "train_imgs = np.array([])\n",
        "train_labels = np.array([])\n",
        "val_imgs = np.array([])\n",
        "val_labels = np.array([])\n",
        "test_imgs = np.array([])\n",
        "test_labels = np.array([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkXbEYJ5c0CK"
      },
      "source": [
        "# load train (30 locations)\n",
        "flag = False\n",
        "for index_name in range(1,4):\n",
        "    print(\"Iteration\", index_name)\n",
        "    if flag == False:\n",
        "        train_imgs = np.load(\"drive/My Drive/data/train_imgs_\" + str(index_name) + \".npy\")\n",
        "        train_labels = np.load(\"drive/My Drive/data/train_labels_\" + str(index_name) + \".npy\")\n",
        "        flag = True\n",
        "    else:\n",
        "        train_imgs = np.concatenate([train_imgs, np.load(\"drive/My Drive/data/train_imgs_\" + str(index_name) + \".npy\")], axis=0)\n",
        "        train_labels = np.concatenate([train_labels, np.load(\"drive/My Drive/data/train_labels_\" + str(index_name) + \".npy\")], axis=0)\n",
        "\n",
        "print(train_imgs.shape)\n",
        "print(train_labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TtF2sdtc0Cg"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbKPagW3mCpY"
      },
      "source": [
        "#### Random images crop\n",
        "\n",
        "> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZUc4o_wc0Ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fa6b83-617d-4ce1-d293-70d9b5ff5ca3"
      },
      "source": [
        "## Cropping - crop only one time each images\n",
        "\n",
        "size = 200\n",
        "\n",
        "tmp_lab = np.zeros((len(train_labels), size, size))\n",
        "tmp_img = np.zeros((len(train_imgs), size, size,3))\n",
        "\n",
        "for k in range(len(train_labels)):\n",
        "\n",
        "  i = np.random.randint(0,len(train_labels[0])-1-size,2)\n",
        "\n",
        "  tmp_lab[k] = train_labels[k,i[0]:i[0]+size, i[1]:i[1]+size]\n",
        "  tmp_img[k] = train_imgs[k,i[0]:i[0]+size, i[1]:i[1]+size]\n",
        "\n",
        "tmp_img = tmp_img.astype(np.uint8)\n",
        "tmp_lab =tmp_lab.astype(np.uint8)\n",
        "\n",
        "print(tmp_lab.shape)\n",
        "print(tmp_img.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(707, 200, 200)\n",
            "(707, 200, 200, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M3NbROwmNG3"
      },
      "source": [
        "\n",
        "#### Data augmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADg9e5nxmBtE"
      },
      "source": [
        "# Numbers of new images: Double the dataset\n",
        "\n",
        "patches_imgs = tmp_img\n",
        "patches_labels = tmp_lab\n",
        "\n",
        "extand_img = patches_imgs\n",
        "extand_lab = patches_labels\n",
        "\n",
        "for k in range(len(patches_imgs)): # Transformation can be mixed\n",
        "\n",
        "  if tf.random.uniform((),minval=0,maxval=1) < 0.25: ## Contrast : (x - mean) * contrast_factor + mean\n",
        "    factor = np.random.uniform(0.4,0.9)\n",
        "    mean = np.mean(patches_imgs[k])\n",
        "    new_image = np.clip(mean + factor * patches_imgs[k] - factor * mean, 0, 255).astype(np.uint8)\n",
        "    new_label = patches_labels[k]\n",
        "\n",
        "  elif tf.random.uniform((),minval=0,maxval=1) < 0.25: ## flipping left rigth / np.flip(image,axis = 1)\n",
        "    new_image = np.flip(patches_imgs[k],axis=1)\n",
        "    new_label = np.flip(patches_labels[k],axis=1)\n",
        "\n",
        "  elif tf.random.uniform((),minval=0,maxval=1) < 0.25: ## flipping up down / np.flip(image,axis = 0)\n",
        "    new_image = np.flip(patches_imgs[k],axis=0)\n",
        "    new_label = np.flip(patches_labels[k],axis=0)\n",
        "\n",
        "  else:\n",
        "    tmp = np.random.randint(1,3) ## Rotation / np.rot90(image,k=..)\n",
        "    new_image = np.rot90(patches_imgs[k],k=tmp)\n",
        "    new_label = np.rot90(patches_labels[k],k=tmp)\n",
        "\n",
        "\n",
        "  extand_img = np.concatenate([extand_img, np.expand_dims(new_image,axis=0)],axis=0)\n",
        "  extand_lab = np.concatenate([extand_lab,np.expand_dims(new_label,axis=0)],axis=0)\n",
        "\n",
        "extand_img.dtype, extand_lab.dtype, extand_img.shape, extand_lab.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgRn1l7KeKv_"
      },
      "source": [
        "plt.imshow(extand_img[0],cmap=\"gray\")\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jZAbj2khc2-"
      },
      "source": [
        "#### Countour reinforcement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqUmBK3KjHX8"
      },
      "source": [
        "import scipy\r\n",
        "from scipy.ndimage import convolve\r\n",
        "\r\n",
        "k = np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\r\n",
        "a = convolve(tmp_img[0,:,:,0], k)\r\n",
        "plt.imshow(a.astype(np.uint8),cmap=\"gray\")\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3At93ztc0DJ"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdahjoBI2TXy"
      },
      "source": [
        "size = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1onUP7c0DL"
      },
      "source": [
        "img_input = tf.keras.layers.Input(shape=(size, size, 3)) # classical unet\n",
        "#img_input = tf.keras.layers.Input(shape=(2,img_size, img_size, 3))#shape=(img_size, img_size, 3))\n",
        "#img_input_2 = tf.keras.layers.Input(shape=(img_size, img_size, 3))#shape=(img_size, img_size, 3))\n",
        "\n",
        "#encoder\n",
        "conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)\n",
        "conv1 = layers.Dropout(0.2)(conv1)\n",
        "conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "pool1 = layers.MaxPooling2D((2, 2))(conv1) # classical unet\n",
        "#pool1 = layers.MaxPooling3D((1, 2, 2))(conv1)\n",
        "\n",
        "\n",
        "conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "conv2 = layers.Dropout(0.2)(conv2)\n",
        "conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "pool2 = layers.MaxPooling2D((2, 2))(conv2) # classical unet\n",
        "#pool2 = layers.MaxPooling3D((1, 2, 2))(conv2) # classical unet\n",
        "\n",
        "#bottleneck\n",
        "conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "conv3 = layers.Dropout(0.2)(conv3)\n",
        "conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "\n",
        "#decoder\n",
        "up1 = layers.concatenate([layers.UpSampling2D((2, 2))(conv3), conv2], axis=-1)  # classical unet\n",
        "#up1 = layers.concatenate([layers.UpSampling3D((1, 2, 2))(conv3), conv2], axis=-1)\n",
        "conv4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up1)\n",
        "conv4 = layers.Dropout(0.2)(conv4)\n",
        "conv4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
        "\n",
        "up2 = layers.concatenate([layers.UpSampling2D((2, 2))(conv4), conv1], axis=-1)  # classical unet\n",
        "#up2 = layers.concatenate([layers.UpSampling3D((1, 2, 2))(conv4), conv1], axis=-1)\n",
        "conv5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up2)\n",
        "conv5 = layers.Dropout(0.2)(conv5)\n",
        "conv5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "out = layers.Conv2D( 1, (1, 1) , activation='sigmoid', padding='same')(conv5)\n",
        "out = layers.Reshape((size, size, 1))(out) # classical unet\n",
        "#out = layers.Reshape((2, img_size, img_size, 1))(out)\n",
        "\n",
        "model = models.Model(inputs=img_input, outputs=out)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URN-MvUac0DY"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\") # tester mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDMy3gKc0Do"
      },
      "source": [
        "history = model.fit(x = extand_img[:707]/255, y = extand_lab[:707]/255, epochs=100) #validation_data=(val_imgs[:64], val_labels[:64]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJrfx1yc0Du"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REWotMXqc0Dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df21592-4a21-4845-b4cc-6c6fca1f2179"
      },
      "source": [
        "# !!! LOADING BOTH TRAINING AND TEST DATA CAN CAUSE THE NOTEBOOK TO CRASH (DEPENDS ON RAM) !!!\n",
        "\n",
        "# load test (20 locations) \n",
        "flag = False\n",
        "for index_name in range(1,3):\n",
        "    print(\"Iteration\", index_name)\n",
        "    if flag == False:\n",
        "        test_imgs = np.load(\"drive/My Drive/data/test_imgs_\" + str(index_name) + \".npy\")\n",
        "        test_labels = np.load(\"drive/My Drive/data/test_labels_\" + str(index_name) + \".npy\")\n",
        "        flag = True\n",
        "    else:\n",
        "        test_imgs = np.concatenate([test_imgs, np.load(\"drive/My Drive/data/test_imgs_\" + str(index_name) + \".npy\")], axis=0)\n",
        "        test_labels = np.concatenate([test_labels, np.load(\"drive/My Drive/data/test_labels_\" + str(index_name) + \".npy\")], axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1\n",
            "Iteration 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiYyK7ht2rjl"
      },
      "source": [
        "# Resize test images to 200x200\r\n",
        "size = 200 \r\n",
        "\r\n",
        "tmp_lab_test = np.zeros((len(test_labels), size, size))\r\n",
        "tmp_img_test = np.zeros((len(test_imgs), size, size,3))\r\n",
        "\r\n",
        "for k in range(len(test_labels)):\r\n",
        "\r\n",
        "  i = np.random.randint(0,len(test_labels[0])-1-size,2)\r\n",
        "\r\n",
        "  tmp_lab_test[k] = test_labels[k,i[0]:i[0]+size, i[1]:i[1]+size]\r\n",
        "  tmp_img_test[k] = test_imgs[k,i[0]:i[0]+size, i[1]:i[1]+size]\r\n",
        "\r\n",
        "tmp_img_test = tmp_img_test.astype(np.uint8)\r\n",
        "tmp_lab_test =tmp_lab_test.astype(np.uint8)\r\n",
        "\r\n",
        "print(tmp_lab_test.shape)\r\n",
        "print(tmp_img_test.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExcoFjTb2H99"
      },
      "source": [
        "#load prediction from U-net\r\n",
        "res = model.predict(tmp_img_test)\r\n",
        "res.shape, len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIJ3iHJW7JGv"
      },
      "source": [
        "# Tranform prediction from U-net to binary images\r\n",
        "# Compared predicted with ground truth\r\n",
        "threshold = 0.5\r\n",
        "bin_img = np.zeros(np.array([len(res),size,size]))\r\n",
        "Err = np.zeros(len(res))\r\n",
        "\r\n",
        "for k in range(len(res)):\r\n",
        "  bin_img[k][res[k,:,:,0] > threshold] = 1\r\n",
        "\r\n",
        "for k in range(len(res)):\r\n",
        "  for i in range(len(res[0])):\r\n",
        "    for j in range(len(res[0])):\r\n",
        "      Err[k] = Err[k] + np.abs(bin_img[k,i,j]-tmp_lab_test[k,i,j]/255)\r\n",
        "\r\n",
        "ratio_err = np.mean(Err)/40000\r\n",
        "\r\n",
        "plt.imshow(bin_img[0,:,:], cmap=\"gray\"), bin_img.shape #0 = black"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1j7f7fH4WGj"
      },
      "source": [
        "plt.imshow(tmp_img_test[81],cmap=\"gray\")\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3zu0Ogo-89b"
      },
      "source": [
        "plt.imshow(tmp_lab_test[81],cmap=\"gray\")\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5cl_Wwv-82M"
      },
      "source": [
        "plt.imshow(res[81,:,:,0],cmap=\"gray\")\r\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}